---
title: 'MUSA500 Homework 5: K-Means Cluster'
author: "Ling Chen, Hang Zhao, Jiahang Li"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    latex_engine: xelatex
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: true
    code_download: true
    theme: united
    highlight: espresso
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
#update.packages(ask = FALSE, checkBuilt = TRUE)
#tinytex::tlmgr_update()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/jiahangl/OneDrive - PennO365/data mining/A5")
options(scipen=999)
options(knitr.table.format = "xelatex")
```    
    
---
```{r setup2, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(NbClust)
library(flexclust)
library(dplyr)
library(sf)
library(ggplot2)
library(RColorBrewer)
library(classInt)
library(cowplot)
library(tinytex)
library(knitr)

#setwd("C:/Users/jiahangl/OneDrive - PennO365/data mining/A5")
data <- read.csv("C:/Users/jiahangl/OneDrive - PennO365/data mining/A5/RegressionData.csv")
df <- data.frame(scale(data[-1:-1]))
head(df)
#palette5 <- c("#AED6F1","#85C1E9","#3498DB","#2874A6","#1B4F72")

```
  



# Introduction
Philadelphia, renowned for its rich history and pulsating present, is home to a continually evolving real estate environment. Given the essential nature of housing as a basic human need, it’s always helpful to understand the real estate market in regions by different categories. Based on previous studies on the relationship between house value and several housing characteristics, as well as the spatial autocorrelations of housing market, we further investigate into data-driven partitioning techniques designed to group the housing market in Philadelphia into similar categories.

In this project, we’ll use cluster analysis to help result in well-interpretable and well-separable groups of objects under consideration. Specifically, the goal is to group block groups in Philadelphia into many different clusters based on socio-economic & housing characteristics from the US census. We’ll use K-means clustering to analyze the large data set of housing market, and to assign all the census blocks to distinct groups with similar socio-economic and housing attributes, i.e., median house value, median household income, percentage of individuals with at least a bachelor’s degree, percentage of singe/detached housing units, and percentage of vacant housing units. Through this, we aim to gain deeper insights into the dynamics and trends within Philadelphia’s housing market, particularly concerning socio-economic factors. Moreover, the resulting clustering categories have the potential to serve as valuable references for shaping future strategies and aiding decision-making processes, which might be super helpful for a wide range of stakeholders, including investors, developers, and policymakers.

# Methods
K-means algorithm is often used for numeric variables and large data set. It helps to divide objects into non-overlapping subsets such that each one is in exactly one cluster. Typically, the number of clusters is specified by a researcher or analyst in advance before running the clustering algorithm. Specifically, there’re 6-step iterative process. 
Firstly, randomly select K data points as cluster centers. Then, calculate the distance between each data point and K cluster centers. After that, assign each data point to a cluster whose distance from the cluster center is minimal among all cluster centers. And after all data points are assigned, recalculate new cluster centers. Then, update the distance between each data point and new cluster centers. If no data point was reassigned, stop. Otherwise, repeat from step 3. 

Still, there are a lot of limitations and problems of K-Means. First and foremost, there’s necessity to specify the number of clusters(K) in advance. Although there’re dozens of methods which statisticians and data mining experts use to identify the best number of clusters, they often disagree about what the optimal number of clusters should be. Also, problems might occur when clusters are of differing sizes, densities, and on-globular shapes. Besides, K-Means is unable to handle noisy data and outliers as well. Last but not least, the final clustering solution will be incorrect as k-means will find the local minimum of SSE, rather than the global minimum.

There are also other types of clustering algorithms, for example, hierarchical clustering and density-based clustering. 
Hierarchical clustering often works well for numeric variables & small data sets and doesn’t need to input the number of clusters. Within the algorithm, a set of nested clusters will be organized as a hierarchical tree. There are two approaches. The first one ‘Agglomerative’ is a bottom-up approach, where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Another approach ‘Divisive’ is a top-down one where all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
Density-based spatial clustering of applications with noise, named DBSCAN, is one of the most common clustering algorithms and can identify irregular cluster shapes. Given a set of points, the algorithm groups together observations that have many neighbors nearby. The points that are alone in low-density regions whose nearest neighbors are too far away are marked as outliers. Specifically, there are two parameters which can be set by the user. One is the Epsilon, which is the radius of neighborhood around a point x, and MinPts, which is the minimum number of neighbors within radius eps that are needed to form a cluster.


# Result
## Number of Clusters - Scree Plot
An ideal clustering solution can be identified where the decline in the Sum of Squared Errors (SSE) significantly diminishes, resulting in an "elbow" shape on the graph plotting SSE against the number of clusters. The graph demonstrates a noticeable reduction in the within-group sum of squares as we move from 1 to 3 clusters. Beyond 3 clusters, the reduction in SSE becomes less pronounced, indicating that a solution with 3 clusters might effectively represent the data.



```{r, warning = FALSE, message = FALSE,echo=FALSE, results='markup'}
#Determine the number of clusters
wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:20) wss[i] <- sum(kmeans(df, centers=i)$withinss)
plot(1:20, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```

## NbClust Approach for Determining Optimal Number of Clusters

We also run the 'NbClust' package in R to determine the cluster number. The 'NbClust' package offers a diverse range of 30 methods to determine the optimal number of clusters. Subsequently, we select the number of clusters that garners the highest consensus among these indices.In this context, we observe that the 3-cluster solution is advocated by the largest number of methods, a consensus that corroborates the findings of the Scree Plot.

```{r, warning = FALSE, message = FALSE}
set.seed(1234)
nc <- NbClust(df,min.nc=2,max.nc=15,method="kmeans",index="all")
table(nc$Best.n[1,])
barplot(table(nc$Best.n[1,]),xlab="Number of Clusters", ylab="Number of Criteria",main="Numebr of Clusters Chosen by 26 Criteria")
```
## Characteristics of Clusters
```{r, warning = FALSE, message = FALSE,echo=FALSE, results='markup',include=FALSE}
set.seed(1234)
fit.km <- kmeans(df,3,nstart=25)
fit.km$size
```

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup',include=FALSE}
round(fit.km$centers, 2)
fit.km$cluster
```
Cluster 1 is characterized by a modest mean median house value of $35,292 and a median household income averaging $20,996. Educational attainment here is relatively low, with only 6.5% of residents possessing at least a bachelor’s degree. This cluster also exhibits a higher percentage of vacant housing units (19.4%) and a minimal presence of single/detached housing units at 7.5%. In contrast, Cluster 3 stands out with the highest mean median house value among the clusters at $191,204, coupled with a substantial median household income of $58,111. Educational levels are notably high, with 55.5% of residents having at least a bachelor’s degree, the highest proportion among all clusters. Also, it has a lower average percentage of vacant housing units (5.8%) and a similar percentage of single/detached housing units (6.9%) compared to Cluster 1.
Cluster 2 presents a moderate profile with a mean median house value of $69,041 and a median household income of $35,372. The percentage of residents with at least a bachelor’s degree is 16.8%, which is higher than Cluster 1 but lower than Cluster 3. It shares the lowest percentage of vacant housing units (5.6%) with Cluster 4 and has a moderately higher presence of single/detached housing units at 4.7%. 

These clusters offer a nuanced view of the varying socio-economic and housing characteristics within the dataset. Based on the statistics, clusters 1-3 will be named as working class housing for value, gentrified housing, and middle-class housing, respectively. 
 
```{r, warning = FALSE, message = FALSE,echo=FALSE, results='markup'}
cbind(round(aggregate(data[-1:-1], by=list(cluster=fit.km$cluster), mean),1),fit.km$size)
```

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
data$cluster_ID <- fit.km$cluster
write.csv(data, "clustered_data.csv", row.names = FALSE)
```

## Merging csv to shapefile & Mapping

The map illustrates Philadelphia's blocks distinctly categorized into three colors: red, yellow, and green, which also demonstrate a generally obvious clustering pattern among various groups, although there are a few blocks scattered across Philadelphia that deviate from this pattern. K-means cluster membership variable seem to be spatial autocorrelated. This is because K-means inherently creates clusters where neighboring blocks are more likely to share similarities, reinforcing the presence of spatial autocorrelation within the clustering process. 

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
# read the original shape file
shp <- st_read("C:/Users/jiahangl/OneDrive - PennO365/data mining/A5/Lecture 1 - Projected Data/Lecture 1 - Projected Data/Regression Data.shp")
```
## Mapping clusters
```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
# delete the duplicate columns in advance
shp <- shp %>% select(-c(6:14))
```

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
# merge on the common column "POLY_ID"
merged <- merge(shp, data, by = "POLY_ID")
```

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
# plot the map
ggplot() +
  geom_sf(data = merged, aes(fill = cluster_ID)) +
  scale_fill_distiller(palette = "RdYlGn")
```
To better compare the clustering map with the original dataset variables, we plot the five maps showing median house value (MEDHVAL), median house income (MEDHHINC), percentage of bachelor's degree (PCTBACHMOR), percentage of single household (PCTSINGLES), and percentage of vacant housing (PCTVACANT), which are shown below: 

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
breaks <- classIntervals(c(min(merged$MEDHVAL), merged$MEDHVAL), n=3, style = "quantile")
```

```{r warning=FALSE, message=FALSE, cache=FALSE,echo=FALSE, results='markup'}
mutated <- mutate(merged, MEDHVAL_cut = cut(MEDHVAL, breaks$brks))
mutated$val_cutted <- cut(mutated$MEDHVAL, breaks = quantile(mutated$MEDHVAL, probs = 0:3 / 3), labels = c("Low", "Medium", "High"))
map1 <- ggplot() +
  geom_sf(data = mutated, aes(fill = val_cutted)) + 
  scale_fill_manual(values = c("#1a9641", "#ffffb2", "#d7191c"))

merged$inc_cutted <- cut(merged$MEDHHINC, breaks = quantile(merged$MEDHHINC, probs = 0:3 / 3), labels = c("Low", "Medium", "High"))
map2 <- ggplot() +
  geom_sf(data = merged, aes(fill = inc_cutted)) +
  scale_fill_manual(values = c("#1a9641", "#ffffb2", "#d7191c"))

merged$mor_cutted <- cut(merged$PCTBACHMOR, breaks = quantile(merged$PCTBACHMOR, probs = 0:3 / 3), include.lowest = TRUE, labels = c("Low", "Medium", "High"))
map3 <- ggplot() +
  geom_sf(data = merged, aes(fill = mor_cutted)) +
  scale_fill_manual(values = c("#1a9641", "#ffffb2", "#d7191c"))

merged$sin_cutted <- cut(merged$PCTSINGLES, breaks = quantile(merged$PCTSINGLES, probs = 0:3 / 3), include.lowest = TRUE, labels = c("Low", "Medium", "High"))
map4 <- ggplot() +
  geom_sf(data = merged, aes(fill = sin_cutted)) +
  scale_fill_manual(values = c("#1a9641", "#ffffb2", "#d7191c"))

merged$vac_cutted <- cut(merged$PCTVACANT, breaks = quantile(merged$PCTVACANT, probs = 0:3 / 3), include.lowest = TRUE, labels = c("Low", "Medium", "High"))
map5 <- ggplot() +
  geom_sf(data = merged, aes(fill = vac_cutted)) +
  scale_fill_manual(values = c("#1a9641", "#ffffb2", "#d7191c"))

map1
```

```{r echo=FALSE, fig.height=12, fig.width=16,echo=FALSE, results='markup'}
plot_grid(map2, map3, map4, map5, 
          align = c("hv","hv","hv","hv"), 
          ncol = 2, nrow = 2)
```

Looking at those maps together, a clear correlation emerges between the clustering patterns and the four variables: median house value, median house income, percentage of getting Bachelor's degree, percentage of vacant housing, where the cluster group 3 includes the blocks having high median house value, high median house income, high percentage of getting Bachelor's degree, low percentage of vacant household, and cluster group 1 includes the blocks having low median house value and income, low percentage of having Bachelor's degree, high percentage of vacant household. As for the percentage of single housing, it does not exhibit a consistent pattern in relation to the cluster groups and is spread throughout the city. Therefore, no associations or cluster reasoning can be found. 

The name for the cluster generally might be: 1 for *'Economically Accessible Housing'*, 2 for *'Middle-Class Housing'*, and 3 for *'Upscale Residential Housing'*. 

# Discussion

In conclusion, we performed clustering method, a set of data-driven partitioning techniques designed to group a collection of objects into clusters, on a dataset of five variables: MEDHVAL, MEDHHINC, PCTBACHMOR, PCTSINGLES, PCTVACANT. This method, similar to unsupervised learning, does not include any information about number of clusters and class label a priori. Looking at the scree plot, we decided to choose 3 as the number of clusters. After conducting the k-means clustering, each row of data was assigned a number of cluster (e.g. 1, 2, or 3), where each cluster share strong association or similarity between members of the same cluster (e.g. high median house value). From the map, similar patterns can be observed between the cluster map and the MEDHVAL, MEDHHINC, PCTBACHMOR maps, and is inversely similar to the PCTVACANT map. However, no obvious association can be seen for PCTSINGLES, as it is more random and is not a factor that strongly related to housing values. 
